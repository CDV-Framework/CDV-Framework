# Case Study Template - Usage Guide

## Quick Start

**Time to complete:** 20-40 hours depending on data availability

**Minimum requirements:**
- Platform documentation
- Basic participation statistics
- 2-3 evaluators for scoring
- Access to users/officials (interviews recommended but not required)

---

## Step-by-Step Process

### Phase 1: Preparation (4-6 hours)

**1.1 Form Evaluation Team**
- Minimum: 3 people
- Ideal composition:
  - 1 government official/platform admin
  - 1 citizen user
  - 1 independent researcher
- Why diversity matters: Different perspectives prevent bias

**1.2 Gather Basic Information**
- Platform name, location, launch date
- Governing documents (laws, decrees, policies)
- User statistics (registered, active, proposals, etc.)
- Any existing evaluations or studies

**1.3 Set Scope**
- Time period to evaluate (e.g., 2020-2024)
- Which processes to include (all vs sample)
- Language(s) of case study

---

### Phase 2: Data Collection (10-15 hours)

**2.1 Platform Data (2-3 hours)**
Access platform itself:
- Create account, explore features
- Download statistics if available
- Screenshot key features/pages
- Test accessibility (WCAG check)

**2.2 Official Documentation (3-4 hours)**
Collect:
- Launch announcements
- User guides
- Annual reports
- Policy documents
- Budget information

**2.3 Academic/Media Sources (3-4 hours)**
Search:
- Google Scholar: "[platform name] participation"
- News archives: Coverage of platform
- Social media: User discussions
- Academic databases (if available)

**2.4 Primary Data (Optional, 4-6 hours)**
If resources allow:
- 5-10 user interviews (30 min each)
- 3-5 official interviews
- Platform observation session
- User survey (can use template)

---

### Phase 3: CDV Evaluation (8-12 hours)

**For each dimension (1-1.5 hours per dimension):**

**Step 1: Individual Scoring (20 min)**
- Each evaluator reads scale descriptions
- Scores 1-5 independently
- Writes brief rationale

**Step 2: Group Discussion (30 min)**
- Share scores
- Discuss discrepancies
- Present evidence

**Step 3: Consensus (10-15 min)**
- Agree on final score (or document split)
- Write final rationale
- List supporting evidence

**Pro tip:** Start with easiest dimensions (often Integrity/Sustainability) to build momentum.

---

### Phase 4: Analysis & Writing (6-10 hours)

**4.1 Draft Core Sections (4-6 hours)**
Priority order:
1. CDV Evaluation (most important)
2. Evidence & Data
3. Context & Background
4. Platform Description
5. Lessons Learned

**4.2 Comparative Analysis (1-2 hours)**
Compare with:
- Similar platforms (same country/region)
- Best practices (Barcelona, Reykjavik)
- Your own benchmarks

**4.3 Recommendations (1-2 hours)**
Based on:
- Low-scoring dimensions (what to improve)
- High-scoring dimensions (what to maintain)
- Comparative insights (what others do better)

---

### Phase 5: Review & Finalize (2-4 hours)

**5.1 Internal Review**
- All evaluators read full draft
- Check consistency
- Verify sources
- Confirm data accuracy

**5.2 External Review (Optional)**
- Share with platform administrators (right of response)
- Share with academic peers
- Incorporate feedback

**5.3 Final Checks**
- All references complete?
- All tables/charts labeled?
- Consistent formatting?
- Executive summary accurate?

---

## Common Challenges & Solutions

### Challenge 1: "We don't have demographic data"

**Solution:**
- Note absence in Inclusion dimension (lowers score)
- Use proxy indicators (geographic distribution if available)
- Qualitative observations from interviews
- Compare with census data (who's missing?)

**In text:**
> "Note: Platform does not collect demographic data. Inclusion score based on qualitative observations and comparison with municipal census data showing likely underrepresentation of [groups]."

---

### Challenge 2: "Officials won't share data"

**Solution:**
- Request under freedom of information laws (if applicable)
- Use publicly available data only (lower depth but valid)
- Note data limitations in methodology
- Compare with leaked/reported figures

**Adjust scoring:**
If data unavailable for dimension:
- Score based on available evidence
- Add asterisk: "Score provisional pending data access"
- Lower confidence in final CDV score

---

### Challenge 3: "Evaluators disagree on scores"

**Solution:**
- Document split scores (e.g., "Score: 3-4, evaluators split 2-1")
- Explain disagreement in rationale
- Both perspectives valuable (shows complexity)
- Final score can be average or range

**Example:**
> "**Score: 3-4** (Evaluators split)  
> Majority (2/3) scored 4 based on [evidence]. One evaluator scored 3 citing [concern]. This reflects genuine ambiguity in [dimension]."

---

### Challenge 4: "Platform too new for long-term evaluation"

**Solution:**
- Focus on design and short-term dimensions (1, 2, 3, 4, 6)
- Score sustainability based on *plans* not outcomes
- Note limitation in Co-Evaluation section
- Plan follow-up evaluation in 2-3 years

**Scoring:**
- Dimension 5 (Co-Evaluation): May only have short-term data
- Dimension 7d (Sustainability): Score based on:
  - Legal framework (exists or not)
  - Budget commitments (multi-year or annual)
  - Political statements (reliable or not)

---

### Challenge 5: "My platform is different from examples"

**Solution:**
- Template is adaptable!
- Modify sections as needed
- Add context-specific content
- Remove non-applicable parts

**Common adaptations:**
- **National vs Municipal:** Add section on federal structure
- **Developed vs Developing:** Expand digital divide analysis
- **Sector-specific:** (health, education) Add domain context
- **Pilot vs Established:** Emphasize learning vs outcomes

---

## Quality Checklist

### Minimum Viable Case Study

**Must have:**
- [ ] All 7 CDV dimensions scored with rationale
- [ ] Evidence cited for each score
- [ ] Basic participation statistics
- [ ] Context (why/how platform created)
- [ ] At least 5 references

**Total:** ~5,000-8,000 words, ~20-30 pages

---

### Comprehensive Case Study

**Should have (in addition to above):**
- [ ] Comparative analysis (2+ platforms)
- [ ] Primary data (interviews, surveys, or observations)
- [ ] Longitudinal data (2+ years)
- [ ] Lessons learned with transferability analysis
- [ ] Recommendations (high/medium/long-term)
- [ ] Demographic analysis
- [ ] 10+ academic references

**Total:** ~15,000-25,000 words, ~60-100 pages

---

### Gold Standard Case Study

**Exemplary (adds to comprehensive):**
- [ ] Multiple data sources triangulated
- [ ] Mixed methods (quant + qual)
- [ ] Multi-stakeholder evaluation team
- [ ] Peer-reviewed
- [ ] Radar chart visualization
- [ ] Raw data in appendix
- [ ] Replication package (data, code if applicable)
- [ ] 20+ sources

**Example:** Barcelona Decidim case study in this repository

---

## Tips for Different User Types

### For Government Officials

**Focus on:**
- Dimension 2 (Co-Agenda): Are you actually sharing power over what gets discussed?
- Dimension 7d (Sustainability): How do you ensure platform survives political transitions?
- Lessons Learned: What can you replicate from best practices?

**Use case study for:**
- Justifying budget allocation
- Demonstrating international standards
- Identifying improvement priorities
- Responding to civic criticism

**Pro tip:** Be honest about low scores. Transparency builds trust.

---

### For Researchers

**Focus on:**
- Methodological rigor (see main CDV Framework documentation)
- Theoretical grounding (link to literature)
- Comparative analysis
- Validity/reliability of scoring

**Use case study for:**
- PhD/Master's thesis chapter
- Journal article (condense to 8,000 words)
- Comparative study dataset
- Teaching material

**Pro tip:** Collaborate with practitioners for data access + legitimacy.

---

### For Civil Society / NGOs

**Focus on:**
- Dimension 6 (Inclusion): Who's excluded and why?
- Dimension 7b (Accountability): Can you hold government to account?
- Recommendations: What to demand from officials
- Lessons Learned: What works elsewhere

**Use case study for:**
- Advocacy (demand improvements)
- Public education (show what's possible)
- Alternative proposals (here's how to do it better)
- Coalition building (share with other organizations)

**Pro tip:** Use low scores to demand action, high scores to celebrate (and maintain).

---

### For Platform Developers

**Focus on:**
- Dimension 3b (Co-Governance): Is your platform democratically governed?
- Dimension 4 (Co-Analysis): Do you provide data access tools?
- Dimension 7a (Transparency): Is your code open source?
- Lessons Learned: What technical features enable democracy?

**Use case study for:**
- Product development priorities
- Demonstrating social impact to funders
- Open-source community building
- User recruitment (show you're serious about democracy)

**Pro tip:** Democratic governance isn't just featuresâ€”it's how you decide what features to build.

---

## Contribution to Repository

### Before Submitting

**1. Self-check:**
- All sections complete (or marked N/A if truly not applicable)
- Sources cited properly (APA format)
- No personal information disclosed (privacy!)
- Honest about limitations
- Balanced (strengths AND weaknesses)

**2. File naming:**
`[Platform-Name]-[Country]-CDV-Case-Study-[Year].md`

Example: `Decidim-Barcelona-Spain-CDV-Case-Study-2025.md`

**3. Include:**
- Main case study (.md file)
- Any supporting data (separate files in `/data` folder)
- Images/charts (in `/images` folder)
- All labeled with same prefix as case study

---

### Submission Process

**1. Fork repository**
```bash
git clone https://github.com/CDV-Framework/CDV-Framework
cd CDV-Framework/case-studies
```

**2. Add your case study**
```bash
mkdir [Platform-Name]-[Country]
cd [Platform-Name]-[Country]
# Add your files
```

**3. Create Pull Request**
- Title: "Case Study: [Platform Name], [Country]"
- Description: Brief summary (3-4 sentences)
- Tag: #case-study

**4. Review process**
- Maintainers review for:
  - Methodological soundness
  - Evidence quality
  - CDV framework proper application
  - Writing quality
- Feedback within 2 weeks
- Revisions welcome

**5. Acceptance**
- Merged into main repository
- You credited as author
- Featured in case study gallery
- Cited in future CDV Framework publications

---

## After Publication

### Share your work

**Recommended channels:**
- Academic: Present at conferences, submit to journals
- Government: Share with officials, inform policy
- Civil society: Use for advocacy
- Media: Write op-eds, press releases
- Social media: Twitter, LinkedIn with #CDVFramework

### Get feedback

**Monitor:**
- GitHub issues (people may comment)
- Citations (Google Scholar alerts)
- Media coverage
- Platform improvements (did your recommendations get implemented?)

### Update if needed

**When to update:**
- Major platform changes
- New data available
- Follow-up evaluation (1-2 years later)
- Errors discovered

**How to update:**
- Submit new Pull Request
- Version number (e.g., v1.0 â†’ v1.1)
- Changelog at top of document

---

## FAQ

**Q: How long does a case study take?**
A: Minimum 20 hours (basic), ideally 40+ hours (comprehensive). Depends on data availability and depth desired.

**Q: Do I need permission to evaluate a platform?**
A: No permission needed for public platforms using public data. Professional courtesy to inform administrators, but not required. If using interviews/private data, obtain consent.

**Q: What if I can't score a dimension due to lack of data?**
A: Note it in rationale, exclude from CDV score calculation, or score conservatively with asterisk indicating uncertainty.

**Q: Can I evaluate my own platform?**
A: Yes, but:
- Disclosure: Note conflict of interest
- Include external evaluators on team
- Expect higher scrutiny in peer review
- Consider independent validation

**Q: My case study disagrees with existing evaluations. Is that okay?**
A: Yes! CDV may reveal different insights than other frameworks. Explain differences in methodology. Disagreement advances knowledge.

**Q: Can I adapt the template?**
A: Absolutely. Template is starting point. Adapt to your context, but keep CDV evaluation core (dimensions 1-7).

**Q: What if my platform scores really low?**
A: Publish anyway! Low scores identify improvement areas. Honesty more valuable than cheerleading. Recommendations section shows path forward.

**Q: I found an error in an existing case study. What do I do?**
A: Open GitHub Issue with:
- What's wrong
- Correct information (with source)
- Suggested fix
Authors will revise or explain if disputed.

---

## Resources

### Templates & Tools

**In this repository:**
- [TEMPLATE-case-study.md](../templates/TEMPLATE-case-study.md) - Blank template
- [Evaluation-Scoring-Sheet.xlsx](../templates/Evaluation-Scoring-Sheet.xlsx) - For team scoring
- [Interview-Protocol.md](../templates/Interview-Protocol.md) - Sample questions
- [User-Survey-Template.docx](../templates/User-Survey-Template.docx) - Questionnaire

**External:**
- WCAG Checker: https://wave.webaim.org/
- Citation Manager: Zotero (free)
- Markdown Editor: Typora, VSCode
- Data Visualization: Plotly, D3.js (for radar charts)

### Learning Resources

**Understanding CDV Framework:**
- [CDV Framework Documentation](https://github.com/CDV-Framework/CDV-Framework) - Read this first!
- [Theoretical Foundations](../research/theoretical-foundations.md)
- [Dimension Guides](../framework/dimensions/) - Detailed explanations

**Case Study Methods:**
- Yin, R. K. (2018). *Case study research and applications*. Sage.
- Gerring, J. (2017). *Case study research: Principles and practices*. Cambridge UP.

**Participatory Evaluation:**
- Cousins, J. B., & Whitmore, E. (1998). Framing participatory evaluation. *New Directions for Evaluation*, 80, 5-23.

### Support

**Questions?**
- GitHub Discussions: https://github.com/CDV-Framework/CDV-Framework/discussions
- Email: almel@eellak.gr
- Monthly Office Hours: First Tuesday of month, 17:00-18:00 CET (Zoom link in discussions)

**Need help?**
- Methodological questions: Post in Discussions
- Technical issues (GitHub, etc.): Create Issue
- Collaboration: Email directly

---

## Version History

**v1.1** (January 2025)
- Corrected dimension references (7a, 7b, 7d instead of Greek letters)
- Updated email contact and repository URLs
- Fixed reference to methodology appendix
- Minor clarity improvements

**v1.0** (January 2025)
- Initial guide
- Based on experience with Barcelona/OpenGov.gr cases

**Future additions:**
- Video walkthrough
- Webinar recordings
- More language versions
- Domain-specific guides (health, education, etc.)

---

**Happy Evaluating! ðŸŽ¯**

*Remember: Perfect case study doesn't exist. Done is better than perfect. Your contribution advances the field.*
